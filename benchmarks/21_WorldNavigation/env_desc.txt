Background:
The Deceptive Grid World is a discrete 10 √ó 10 labyrinth whose visual symbolism is intentionally inverted to undermine an agent‚Äôs prior knowledge. What appears to be treasure is fatal, what looks like danger may be salvation, and seemingly solid walls hide walkable tiles. Each episode is generated by a fresh random seed, but the governing rules never change, preserving a stable learning target beneath the superficial chaos.

Objective:
Within a hard limit of thirty time-steps the agent must locate and step onto the single rewarding ‚ò† tile, which is visually identical to other ‚ò† tiles yet is the only true Safe Zone. Stepping on it yields success and ends the episode. All other outcomes‚Äîtriggering a üí∞ trap or simply running out of steps‚Äîterminate the run without reward.

State Setup:
At reset the environment samples a new grid layout that obeys three immutable placement rules: every üí∞ tile is a lethal trap, exactly one ‚ò† tile is the rewarding Safe Zone while the rest behave like empty floor, and every ‚¨õ tile is traversable despite its appearance. The agent is spawned on an empty ‚óª floor tile chosen uniformly at random. From that moment the global grid is fixed; only the agent‚Äôs position and the countdown of steps remaining evolve with time.

Actions:
Five discrete actions are available at every decision point‚ÄîMoveNorth, MoveSouth, MoveEast, MoveWest and Wait. Movement attempts that would leave the 10 √ó 10 boundary have no spatial effect yet still consume a time-step, mirroring real-world consequences for poor planning. Waiting simply decrements the step budget and leaves the agent in place.

State Transition Rule:
Executing a movement action first decrements the remaining-steps counter, then checks the intended destination. If the destination lies within the grid, the agent‚Äôs coordinates are updated to that cell; otherwise coordinates remain unchanged. After relocation, the environment evaluates the tile now occupied. Occupying the rewarding ‚ò† issues the success reward and flags the episode as terminal, landing on any üí∞ tile triggers immediate terminal failure, whereas all other tiles‚Äîincluding inert ‚ò†, illusory ‚¨õ, and ordinary ‚óª‚Äîallow play to continue until the step limit is exhausted.

Rewards:
The game uses a binary reward scheme. A single positive reward of +1 is emitted once, and only once, when the agent stands on the unique rewarding ‚ò†. All other transitions, including fatal traps, inert tiles and timeout, yield zero. This stark signal forces the learner to infer the true rules through systematic exploration rather than incremental shaping.

Observation:
Each turn the agent receives a centered 5 √ó 5 ego-centric window of grid symbols with the agent glyph üßç always occupying the central cell, accompanied by an integer indicating steps remaining. No absolute coordinates, orientation markers or map memory are provided. This partial view offers actionable local information‚Äîrevealing nearby traps, deceptive walls and potential Safe Zones‚Äîwhile withholding global context, thereby encouraging the development of internal memory or belief representations. Because the symbol-to-behavior mapping is fixed and globally consistent across episodes, the observations contain reliable patterns that a learning algorithm can exploit once it discards misleading semantic priors.

Termination:
An episode concludes immediately when one of three mutually exclusive conditions fires: the agent steps onto the rewarding ‚ò† (success), the agent lands on any üí∞ trap (failure), or the countdown reaches zero after thirty steps (timeout). In all cases the environment returns a terminal flag and prevents further actions, guaranteeing well-defined episodic boundaries.

Special Features:
The environment exposes an optional determinism switch that fixes the random seed, allowing researchers to reproduce entire sequences of layouts and interactions. Although a new grid is rolled every episode by default, the symbol meanings, action repertoire, observation format and step budget remain immutable, ensuring that difficulty arises from spatial uncertainty rather than rule instability. Because ‚¨õ walls are always passable and ‚ò† tiles always have a one-in-N chance of being the goal depending on episode configuration, the learner can form stable, transferable policies. Finally, illegal moves incur a time cost but no negative reward, reinforcing the principle that the sole learning signal stems from discovering genuine safety hidden beneath deceptive appearances.