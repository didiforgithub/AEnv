Background:
The Bizarro Lab Experiments environment places the agent in a fully observable chemical laboratory whose physical and chemical rules are systematically inverted. Heating equipment extracts heat while cooling equipment injects it, reagents labelled “Acid” behave as strong bases and vice-versa, and the direction of all stoichiometric and energetic relationships is reversed. These inversions are immutable and universal across every episode, establishing a coherent yet counter-intuitive world in which the agent must discard prior assumptions and learn the true causal structure through interaction alone.

Objective:
At the start of an episode the environment specifies a target compound and a required purity of at least ninety-five per cent. Within a strict horizon of forty steps the agent must manipulate reagents and equipment so that a beaker containing material meeting or exceeding the purity threshold is placed on the analysis tray via an explicit “Submit for Analysis” action.

State Setup:
When an episode begins, five empty beakers sit on the bench, all equipment (hot plates, cooling coils and magnetic stirrers) is switched off, temperatures equal the ambient laboratory value, and pH values are neutral. The environment supplies a structured, fully observable state vector each step. For every beaker it shows the complete compositional breakdown—expressed as percentage of every base chemical and all emergent intermediates—together with current temperature and pH. It also reports whether each hot plate or cooling coil is active and the stirrer speed setting. A global timer counts down the remaining steps, and the immutable target compound identifier plus its purity requirement appear in the observation for reference. A small, step-to-step stochastic noise of plus or minus one degree Celsius is applied to temperature changes to avoid perfect determinism without obscuring the state.

Actions:
At every time-step the agent chooses exactly one discrete action. It may add a measured volume of a named reagent to a chosen beaker, toggle the hot plate or cooling coil beneath a beaker (remembering that toggling heat actually cools and toggling cooling actually heats), set the magnetic stirrer speed between zero and three, transfer liquid from one beaker to another, discard the contents of a beaker, submit a beaker for final analysis, or simply wait. All actions consume a time-step; illegal moves such as over-filling are ignored yet still advance the clock, signalling the agent that nothing was accomplished.

State Transition Rule:
Every applied action modifies the deterministic laboratory simulator before kinetic evolution is processed. Adding a reagent alters the compositional vector according to the introduced volume. Toggling a hot plate subtracts five degrees Celsius per active step, while toggling a cooling coil adds five degrees Celsius. Stirrer speed influences reaction rates but never contradicts the inverted reaction table. After equipment effects are calculated, the environment consults its fixed reverse-chemistry lookup table to update species concentrations and to determine whether energy is released or absorbed, always following the inverted rules (neutralisation becomes amplification, exothermic signs flip, etc.). These deterministic transformations, combined with the mild temperature noise, yield a world whose dynamics are complex yet strictly learnable.

Rewards:
The environment provides a dense, strictly non-negative, cumulative reward. It tracks the highest purity of the target compound present in any beaker at the current step and compares it with the previous step. Any positive improvement is granted immediately as reward; regressions incur no penalty. Formally, the per-step reward equals the increase in maximum purity, clipped at zero. If the agent submits a beaker whose purity meets or exceeds ninety-five per cent, the episode grants a one-point base bonus plus a time-efficiency bonus equal to half of the unused fraction of the forty-step budget, encouraging both success and speed. Because the reward signal never dips below zero and the underlying laws never change, the agent can confidently interpret reward increments as progress toward the goal.

Observation:
The observation is purposely exhaustive so that logical patterns rather than hidden variables define the challenge. The full compositional vectors reveal how individual reagents contribute to intermediates and final products, allowing the agent to correlate specific additions or temperature manipulations with purity gains. Equipment status and temperature readings help disambiguate whether changes in kinetics arise from stirring or thermal effects. The global step counter supports temporal planning, while the constant display of the target compound reminds the agent of its objective. Hierarchically, high-level cues—purity, pH, temperature—are immediately visible for rapid feedback, whereas low-level percentage breakdowns provide the granular data necessary for long-term strategy formation. This balance ensures that the observations remain information-rich without becoming opaque.

Termination:
An episode ends either when the agent calls SubmitForAnalysis, regardless of success, or when forty steps elapse. Submitting with sufficient purity constitutes success; failing to reach the threshold or timing out constitutes failure. In either case the episode concludes, the final reward is issued and the environment resets.

Special Features:
Several design decisions reinforce learnability. All inversions, reagent lists and reaction tables are invariant across the entire training and evaluation suite, enabling the agent to converge toward a perfect internal model rather than chasing ever-shifting rules. The mild temperature noise injects diversity without destroying determinism. No negative rewards or stochastic penalties exist, so exploration is shaped by positive reinforcement rather than avoidance learning. Illegal actions being ignored—but costly in time—teach implicit constraints while never producing ambiguous penalties. Finally, because difficulty arises from systematic inversion and combinatorial reaction space rather than randomness, mastery depends on discovering consistent patterns, making the environment an ideal benchmark for model-based or model-free reinforcement learning under counter-intuitive but fully coherent physics.