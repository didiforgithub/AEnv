# Masked Pixel Art Completion Environment Design Document

## Background

The Masked Pixel Art Completion Environment presents a creative restoration challenge where an intelligent agent must reconstruct damaged or partially hidden pixel art images. The environment simulates a scenario similar to digital art restoration or puzzle completion, where the agent operates as a digital conservator working to restore corrupted artwork. Each episode begins with a 10×10 pixel canvas representing a recognizable icon or symbol from a curated library containing semantic objects such as cats, trees, arrows, letters, and other common visual motifs. The corruption process randomly masks between 20 to 30 pixels with unknown tokens, creating gaps that the agent must intelligently fill based on contextual clues from the surrounding visible pixels. The environment maintains semantic consistency through a fixed 16-color palette where each color index consistently represents the same visual concept across all episodes, enabling the agent to develop and apply learned associations between colors and their typical contextual meanings.

## Objective

The agent's primary goal is to maximize the cumulative similarity between its restored canvas and the hidden ground-truth image within a strict 40-step time budget. Success is measured not by binary completion but by the total number of correctly identified pixels that were originally masked, encouraging the agent to prioritize high-confidence predictions while exploring efficient navigation strategies. The agent must balance exploration of the canvas space with exploitation of discovered patterns, developing strategies that combine efficient cursor movement with accurate color prediction. The objective promotes learning of both spatial reasoning skills for optimal path planning and pattern recognition capabilities for contextual color inference.

## State Setup

Each episode begins with the environment randomly selecting a ground-truth pixel art image from the predefined library of icons and symbols. The initialization process applies a randomized masking procedure that replaces between 20 to 30 pixels with special unknown tokens, ensuring that sufficient context remains visible while creating meaningful restoration challenges. The agent's cursor is positioned at coordinates (0,0) representing the top-left corner of the canvas. The masked positions and the underlying ground-truth image remain fixed throughout the entire episode, providing a stable learning environment where the agent's actions have deterministic consequences. The 16-color palette maintains consistent semantic mappings across all episodes, where specific color indices always correspond to the same visual concepts such as blue representing water or sky elements, green representing vegetation, and other semantically meaningful associations.

## Actions

The agent operates through a comprehensive action space consisting of 21 distinct actions designed to provide full control over cursor navigation and pixel modification. The movement actions include MoveNorth, MoveSouth, MoveEast, and MoveWest, allowing the agent to navigate freely across the 10×10 canvas coordinate system. Sixteen WriteColor actions enable the agent to place any of the available palette colors at the current cursor position, with each color index maintaining consistent semantic meaning throughout all episodes. The Skip action allows the agent to deliberately leave the current cell unchanged, which can be strategically valuable when the agent determines that a cell was not originally masked or when uncertainty about the correct color is high. Movement attempts that would place the cursor outside the valid coordinate bounds are handled gracefully by maintaining the cursor's current position while still consuming a time step, ensuring that invalid navigation attempts carry an opportunity cost without causing environment errors.

## State Transition Rule

The environment processes each action with deterministic state transitions that provide clear feedback for agent learning. Movement actions update the cursor coordinates according to standard directional conventions, with the coordinate system treating (0,0) as the top-left position and (9,9) as the bottom-right position. When the agent attempts to move beyond the canvas boundaries, the cursor position remains unchanged but the action still consumes one step from the time budget, creating a mild penalty for inefficient navigation without introducing harsh punishment mechanisms. WriteColor actions modify the canvas state by placing the specified color at the current cursor location, regardless of whether that cell was originally masked or contains a visible ground-truth value. The Skip action advances the time step counter without modifying any canvas state, providing the agent with an explicit choice to pass on uncertain decisions. All actions consume exactly one step from the 40-step episode budget, creating consistent temporal costs that encourage efficient action selection and strategic planning.

## Rewards

The environment employs a cumulative reward structure with exclusively positive reinforcement to encourage exploration and learning without punitive feedback for incorrect guesses. The reward mechanism evaluates each WriteColor and Skip action by comparing the resulting cell value against the hidden ground-truth image, awarding exactly +1.0 points when the agent correctly identifies the original value of a previously masked cell. No rewards are granted for correctly writing colors into cells that were never masked, focusing the learning signal specifically on the restoration task rather than redundant copying of visible information. Incorrect color choices and Skip actions on masked cells both result in zero reward, creating a neutral outcome that avoids negative reinforcement while still providing clear feedback about prediction accuracy. The cumulative nature of the reward system means that episode returns directly correspond to the number of correctly restored masked pixels, with the theoretical maximum return equaling the count of initially masked cells for perfect restoration performance.

## Observation

The agent receives carefully structured partial observations designed to support pattern recognition and strategic decision-making while maintaining appropriate task difficulty. The observation space includes the current cursor position as an integer coordinate pair bounded within [0,9], providing explicit spatial awareness for navigation planning. A local 3×3 neighborhood centered on the cursor position reveals the immediate contextual information surrounding the agent's current focus area, with cells displaying either their true color indices (0-15) or the special unknown token (□) for masked positions. A binary indicator vector tracks which of the 16 palette colors have appeared at least once in the currently visible portions of the canvas, helping the agent understand the overall color composition and identify potentially missing elements from the semantic scene. The remaining step count is provided as an explicit integer value from 0 to 40, enabling the agent to adapt its strategy based on time pressure and develop appropriate urgency behaviors as the episode deadline approaches. The observation design deliberately withholds the complete canvas view and the ground-truth target, requiring the agent to build spatial understanding through active exploration while using the provided contextual cues to make informed restoration decisions.

## Termination

Episodes conclude under two distinct conditions that respect both task completion and resource constraints. The primary success termination occurs when the agent has filled all originally masked cells with any color values, regardless of correctness, representing complete coverage of the restoration task. The secondary timeout termination activates after exactly 40 steps have been consumed, ensuring that all episodes conclude within a consistent time frame even if the agent has not addressed all masked cells. Both termination conditions trigger an immediate environment reset that generates a fresh ground-truth image from the library with newly randomized mask positions, preparing for the next episode without requiring explicit external intervention. The termination system does not provide early stopping based on reward thresholds or performance metrics, maintaining consistent episode lengths that support stable learning dynamics and fair performance comparisons across different agent strategies.

## Special Features

The environment incorporates several distinctive design elements that enhance its educational and research value while maintaining strict consistency requirements. The semantic consistency mechanism ensures that color indices maintain identical visual and conceptual meanings across all episodes, training images, and evaluation scenarios, enabling agents to develop transferable knowledge about common-sense color associations and typical pixel art conventions. The deterministic reset functionality supports reproducible research by accepting optional random seed parameters that control both ground-truth image selection and mask position generation, ensuring that experimental results can be replicated and validated across different research contexts. The fixed step budget of exactly 40 actions per episode creates uniform temporal constraints that prevent episodes from becoming excessively long while providing sufficient time for thorough canvas exploration and strategic restoration attempts. The positive-only reward structure eliminates harsh punishment mechanisms while still providing clear performance feedback, supporting stable learning processes that encourage exploration and experimentation without fear of catastrophic performance penalties. The difficulty consistency guarantee ensures that all provided levels maintain identical environmental parameters including canvas dimensions, action spaces, observation formats, and reward calculations, with variation introduced only through different combinations of ground-truth images and masking patterns rather than fundamental rule changes that would impede transfer learning and skill development.