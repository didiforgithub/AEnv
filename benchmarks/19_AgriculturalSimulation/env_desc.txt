Background  
The Backwards Valley Farm Management environment places the learning agent in a 10-by-10 discrete farm where natural laws behave in a deliberately inverted manner. Three distinct zones share the map: crop fields, animal pens, and a small village. Contrary to everyday intuition, biological and social processes thrive on neglect and regress on care. Crops race toward ripeness when left alone and wither when tended; livestock flourish in filth and hunger; villagers warm to the agent only when verbally abused. These reversed dynamics are fixed, identical in every episode, and constitute the central puzzle the agent must master through experience rather than common-sense priors.

Objective  
At every episode the agent receives exactly forty action opportunities and must maximise a continuously accumulating scalar called Farm Value. Farm Value is the sum of three concealed underlying meters—Crop Yield, Animal Health, and Social Affinity—each bounded between zero and one hundred. Because Farm Value is monotonically non-decreasing, the agent’s episodic return equals the final Farm Value or, equivalently, the sum of its step-wise increments.

State Setup  
An episode begins with a fresh map that obeys the same size and encoding on every run while randomising the concrete placement and initial stages of fields, pens, and villagers. The agent materialises at a uniformly sampled empty tile. Each field tile starts in one of four growth stages, each pen in one of three health states, and each villager in one of three social moods. These initial categorical values are drawn from the same distributions across episodes so that learning does not hinge on memorising individual layouts but rather on understanding the invariant inverse rules.

Actions  
The action space is fixed to eleven discrete commands. Four cardinal moves translate the avatar by one tile when the destination is not blocked by a fence. A Wait command explicitly lets a time step pass without interaction, which, because of the reversed causality, can be strategically beneficial. Six directed interaction verbs—UseWateringCan, SpreadFertilizer, Feed, CleanPen, Compliment, and Insult—target the single tile directly in front of the agent and consume the step even when their pre-conditions are unmet. Every action is instantaneous; there is no stamina or inventory resource to deplete.

State Transition Rule  
With each applied action, the environment executes two layers of deterministic logic. First, the local effect of the chosen verb occurs: watering or fertilising a crop, feeding or cleaning an animal, complimenting or insulting a villager, or simply waiting or moving. Second, global background rules tick once for every entity on the board. A crop that did not receive a direct action advances by exactly one stage; receiving any care resets it to Seed. An untouched animal’s health increases by one tier while any direct care downgrades it to Weak. A villager’s mood drifts only in response to explicit compliments or insults, with compliments pushing the mood down and insults pushing it up. When a crop becomes Harvest-Ready and the agent steps onto that tile, it is harvested automatically and removed, instantly injecting Farm Value; similarly, an animal that first arrives in the Thriving state or a villager that first achieves the Friendly mood yields a one-off Farm Value bonus. All transitions are deterministic given the current map, ensuring the system’s learnability stems from structural complexity rather than stochastic noise.

Rewards  
The design uses strictly non-negative, cumulative rewards. At each time step the agent observes ΔV = FarmValue_t – FarmValue_{t-1}. Because no rule ever decreases Farm Value, ΔV is guaranteed to be at least zero. Harvesting a ripe crop awards between two and five points depending on its random base value; witnessing an animal’s first transition to Thriving grants one to three points; witnessing a villager’s first transition to Friendly grants three to six points. No other events generate reward, and no action carries a penalty.

Observation  
After every step the agent receives a partial yet information-rich view of the world. The observation is centred on the agent and spans a five-by-five grid aligned to the global coordinates, conveying for each visible tile its terrain type plus a concise categorical status when the tile is a field, pen, or village house. The agent’s absolute (x, y) location, remaining step budget, and the latest Farm Value accompany this local map. No past observations are stored by the environment; the agent must build its own belief about unseen territory. This hierarchy reveals just enough actionable cues—what is in front, what stage each entity is in, and how much time remains—to let a competent policy emerge while still demanding strategic exploration and memory.

Termination  
An episode halts automatically when forty actions have been executed, or earlier if Farm Value reaches the theoretical maximum of three hundred. Because rewards never decrease and resources are abundant, there is no terminal failure such as bankruptcy or starvation; the clock alone imposes urgency.

Special Features  
Two implementation guarantees preserve consistency and reproducibility. First, all inverse cause-and-effect mechanics are immutable across training, validation, and test seeds, meaning a policy that discovers the benefit of neglect will generalise. Second, a determinism flag can lock the random seed so that academic studies may replicate exact trajectories. The fixed map size, constant observation schema, and action semantics together ensure the environment remains a stable and learnable benchmark whose difficulty arises from counter-intuitive logic rather than from unstructured randomness or hidden parameter shifts.