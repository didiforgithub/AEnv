# Binary Warehouse Box Sorting Environment Design Document

## Background

The Binary Warehouse Box Sorting Environment simulates a classic warehouse logistics scenario where an automated forklift agent must efficiently organize cargo within a structured facility. The environment is built upon the foundational mechanics of Sokoban, a timeless puzzle game that emphasizes spatial reasoning and strategic planning. The warehouse consists of a discrete 10×10 grid representing aisles, storage areas, and loading docks typical of modern distribution centers. Each level presents a unique warehouse layout with strategically placed walls forming corridors, dead-ends, and bottlenecks that mirror real-world warehouse constraints. The environment maintains complete determinism and rule consistency across all difficulty levels, ensuring that learned behaviors transfer seamlessly between different warehouse configurations.

## Objective

The agent's primary mission is to achieve complete warehouse organization by positioning every cargo box onto its designated loading dock within a strict operational timeframe. Success is defined as the placement of all boxes (ranging from 3 to 5 per level) onto available dock tiles before exhausting the 40-step time budget. This binary success condition reflects real-world warehouse operations where partial completion often provides no operational value, and time efficiency directly impacts logistics performance. The agent must develop strategic thinking to avoid irreversible mistakes such as pushing boxes into corners or creating blockages that prevent optimal solutions within the temporal constraints.

## State Setup

Each episode initializes with a deterministically generated 10×10 warehouse layout that remains static throughout the entire episode. The environment randomly selects from a pool of predefined configurations, each containing between 3 to 5 boxes and an equal number of corresponding dock tiles. Wall placement creates realistic warehouse topology with narrow aisles, storage alcoves, and potential dead-end traps that require careful navigation planning. The agent spawns at a designated starting position on an empty floor tile, with all boxes initially placed on regular floor spaces away from their target docks. The step counter begins at 40, representing the maximum allowed operational time, and decrements with each action regardless of its success or failure. This initialization ensures consistent challenge levels while providing sufficient variety to prevent overfitting to specific layouts.

## Actions

The agent operates through four fundamental movement commands that directly control the forklift's directional movement within the warehouse grid. MoveNorth advances the agent one cell toward the top of the grid, MoveSouth moves toward the bottom, MoveEast progresses rightward, and MoveWest advances leftward. Each action attempts to relocate the agent to the target cell in the specified direction. When the target cell contains empty floor or dock space, the agent successfully moves and occupies that position. When the target cell contains a box, the action triggers a push attempt where the box is relocated one cell further in the same movement direction, but only if that destination cell is unoccupied floor or dock space. Actions targeting walls or attempting impossible box pushes (where the box cannot be displaced) result in no state changes but still consume one step from the time budget, adding strategic cost to failed attempts.

## State Transition Rule

State transitions follow deterministic Sokoban mechanics that ensure predictable and learnable environment behavior across all levels. When an agent executes a movement action, the environment first evaluates the legality of the intended move by checking the target cell's contents. For movements into empty spaces, the agent position updates immediately and the step counter decrements. For movements into boxes, the system calculates the box's potential destination and executes the push only if that location is unobstructed, simultaneously updating both agent and box positions. Illegal actions, including movements into walls or pushes where boxes cannot be displaced due to obstacles, leave all entity positions unchanged while still consuming the action step. The environment maintains perfect collision detection and ensures that no two entities ever occupy the same grid cell, with boxes unable to stack or overlap under any circumstances. This deterministic behavior guarantees that identical action sequences from identical states always produce identical outcomes, enabling reliable policy learning and evaluation.

## Rewards

The environment implements a pure binary reward structure that reflects the all-or-nothing nature of successful warehouse operations. The agent receives exactly +1 reward when the final box is successfully placed on a dock tile, completing the warehouse organization task within the 40-step time limit. All other situations, including intermediate steps during task progression, failed actions, and episodes that exhaust the time budget without achieving complete success, yield exactly 0 reward. This non-cumulative reward design eliminates reward shaping and forces the agent to develop long-term strategic planning rather than relying on incremental feedback. The binary nature ensures that partial solutions provide no reward advantage, accurately modeling scenarios where incomplete warehouse organization fails to meet operational requirements and provides no business value.

## Observation

The environment provides complete omniscient observation that eliminates uncertainty and focuses learning on strategic decision-making rather than perception challenges. Every step exposes the full 10×10 warehouse map where each cell is clearly categorized as EmptyFloor, Wall, Box, Dock, BoxOnDock, or Agent, typically encoded as distinct integers or ASCII symbols for easy interpretation. The observation includes the agent's precise absolute coordinates within the grid, enabling accurate spatial reasoning and path planning. Critical task information is provided through the count of boxes still requiring placement on docks, allowing the agent to track progress toward completion. The remaining step count is continuously visible, creating time pressure awareness that influences strategic urgency. This comprehensive observational design ensures agents have sufficient information to identify optimal action sequences while maintaining the core challenge of spatial reasoning and temporal planning inherent in warehouse logistics optimization.

## Termination

Episodes conclude under two specific conditions that reflect natural task boundaries and operational constraints. Successful termination occurs immediately when the agent places the final box onto a dock tile, achieving complete warehouse organization within the allocated timeframe and triggering the +1 reward. Time-based termination activates when the 40-step budget is fully exhausted regardless of task completion status, resulting in 0 reward and immediate episode end. Upon either termination condition, the environment automatically resets with a new warehouse layout selected from the available configuration pool, maintaining identical mechanics and challenge parameters while providing fresh spatial arrangements. This termination design ensures episodes have clear success criteria and finite duration, preventing indefinite wandering while maintaining pressure for efficient solution discovery.

## Special Features

The environment incorporates several distinctive characteristics that enhance its value for reinforcement learning research and evaluation. Step Budget Enforcement rigorously limits every episode to exactly 40 actions, creating consistent time pressure that demands efficient planning and execution across all difficulty levels. Difficulty Consistency Guarantee ensures that all training, validation, and test levels maintain identical grid dimensions, step limits, action sets, observation formats, and reward mechanisms, with variation occurring only in wall, box, and dock placement patterns. Rule Consistency Assurance guarantees that Sokoban pushing mechanics, collision detection, and reward computation remain absolutely unchanged across all levels, enabling policy transfer without adaptation requirements. The No Curriculum Approach deliberately avoids progressive difficulty scaling, instead maintaining comparable challenge levels with 3-5 boxes per level and similar corridor complexity to test generalization capabilities. Reward Non-Negativity strictly prevents negative reward values, with failure states yielding 0 rather than penalties, promoting exploration without punishment-based discouragement. Finally, Reproducibility Support through global seed control enables complete determinism for rigorous benchmarking and scientific reproducibility, allowing exact episode sequences to be replicated across different experimental runs.