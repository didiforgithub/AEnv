**Background**

The Gear Ratio Optimization Environment simulates a mechanical engineering workshop where an agent must design and assemble linear gear trains to achieve specific mechanical advantages. The environment is grounded in real-world mechanical engineering principles, where gears with different tooth counts mesh together to create torque and speed transformations. Each gear is characterized by its tooth count, ranging from 6 to 60 teeth, and when two gears mesh, they create a ratio equal to the quotient of their respective tooth counts. The agent operates as a mechanical engineer who must select and arrange gears from an available library to construct a gear train that meets precise specifications within a limited timeframe.

**Objective**

The agent's primary goal is to construct a linear gear train that achieves a target mechanical advantage within a specified tolerance before exhausting a 30-step action budget. At the beginning of each episode, the environment provides a randomly sampled target mechanical advantage (R_target) and a fixed tolerance of 2%. Success is achieved when the agent executes a "Finish" action and the assembled gear train's mechanical advantage falls within the acceptable range: |MA - R_target| / R_target ≤ 0.02. The challenge lies in selecting the optimal combination and sequence of gears to produce the desired mechanical advantage while managing the constraint of limited assembly steps.

**State Setup**

Each episode begins with the environment randomly generating a gear library containing exactly 10 available gear types, each defined by a tooth count between 6 and 60. The target mechanical advantage R_target is also randomly sampled at episode initialization. The gear chain starts empty, resulting in an initial mechanical advantage of 1.0. The step counter begins at 30, representing the maximum number of actions the agent can take. The tolerance remains fixed at 2% (0.02) across all episodes. This initialization ensures that each episode presents a unique challenge while maintaining consistent rules and constraints that enable systematic learning.

**Actions**

The agent can execute four distinct action types during each step. The PlaceGear[i] action appends the i-th gear from the available library to the end of the current gear chain, where i ranges from 0 to N_available-1. The RemoveLast action removes the most recently placed gear from the chain, allowing the agent to backtrack and explore alternative configurations. The Finish action declares the current assembly complete and triggers the final evaluation against the target mechanical advantage. The Skip action allows the agent to pass without modifying the gear chain, which may be useful for deliberation or when no beneficial action is apparent. Invalid actions, such as attempting to place a gear with an out-of-range index, consume a step but leave the environment state unchanged.

**State Transition Rule**

When the agent executes a PlaceGear[i] action, the selected gear is appended to the gear chain, and the mechanical advantage is recalculated using the formula MA = (T₁ / T₂) × (T₃ / T₄) × ... where consecutive gears in the chain create meshing ratios. The RemoveLast action removes the final gear from the chain and recalculates the mechanical advantage based on the remaining gears. The Skip action advances the step counter without modifying any other state components. The Finish action triggers the termination condition and evaluates whether the current mechanical advantage meets the success criteria. All actions decrement the remaining step counter by one. Invalid actions result in no state changes except for the step counter decrement, ensuring that the environment remains in a valid state while still penalizing incorrect action selections through time consumption.

**Rewards**

The environment employs a binary reward structure with values restricted to {0, 1}. The agent receives a reward of +1 only when it executes the Finish action and the final mechanical advantage satisfies the tolerance requirement: |MA - R_target| / R_target ≤ 0.02. All other situations, including timeout terminations, failed finish attempts, or intermediate steps, result in a reward of 0. This binary reward structure eliminates partial credit and focuses the agent's learning on achieving the precise objective rather than approximate solutions. No negative rewards or penalties are imposed, ensuring that the agent's cumulative reward never decreases and maintaining focus on the single success condition.

**Observation**

The environment provides complete observability of all relevant state information to support effective learning. At each step, the agent observes the gear library as an ordered list of available tooth counts, enabling identification of all possible gear selections. The current gear chain is presented as an ordered sequence showing the exact configuration of placed gears. The current mechanical advantage is provided as a real-time calculation, allowing the agent to understand the immediate impact of each gear placement. The target ratio and tolerance are continuously visible, providing constant reference points for decision-making. The remaining step count creates temporal pressure and informs resource management decisions. This comprehensive observation design ensures that the agent has sufficient information to understand action consequences, evaluate progress toward the goal, and develop strategic approaches to gear selection and sequencing.

**Termination**

Episodes terminate under two specific conditions that provide clear endpoints for learning sequences. Explicit termination occurs when the agent executes the Finish action, regardless of whether the mechanical advantage meets the success criteria. Implicit termination occurs when the step counter reaches zero, indicating that the 30-step budget has been exhausted. Upon termination, the environment immediately resets with a newly sampled gear library and target ratio, ensuring that subsequent episodes present fresh challenges. This termination structure provides natural episode boundaries that align with the task objective while maintaining consistent episode lengths that support batch learning algorithms.

**Special Features**

The environment incorporates several distinctive mechanics that enhance both realism and learning potential. Gear reusability allows the agent to use the same tooth count multiple times within a single gear train, reflecting real-world scenarios where multiple identical gears might be optimal. The deterministic mechanical advantage calculation ensures that identical gear configurations always produce the same results, enabling the agent to learn reliable cause-and-effect relationships. The fixed library size of 10 gears across all episodes provides consistent action space dimensionality while allowing tooth count variation to create diverse challenges. An optional determinism flag enables researchers to fix random seeds for reproducible experiments and systematic analysis. The semantic alignment between gear representations and real-world mechanical principles allows agents to potentially leverage domain knowledge or transfer learning from related mechanical tasks, while the mathematical precision of gear ratio calculations ensures that learned strategies generalize reliably across different configurations.