Background:  
The environment is a discrete, procedurally generated 11 × 11 maze that thematically represents an underground ruin filled with immutable stone walls, pools of water, deadly fire pits, and a single golden treasure chest. The surrounding outer wall seals the ruin, ensuring the agent can neither escape nor see beyond the playable bounds. Although every episode instantiates a fresh map layout, the semantics of each tile type are invariant across time: walls and water are forever impassable, fire is always lethal, and the treasure remains a benign, coveted goal tile. This semantic stability guarantees that knowledge learned in one episode transfers cleanly to the next, allowing agents to rely on consistent world rules while contending with spatial novelty.

Objective:  
The agent’s sole task is to explore the maze and step onto the treasure tile within a strict budget of forty time-steps. Doing so yields immediate success; any other form of termination constitutes failure. There is no concept of score accumulation or partial credit—the mission is binary: find the treasure in time or perish/timeout trying.

State Setup:  
At the beginning of each episode, a deterministic yet seed-controlled map generator carves an outer perimeter wall and fills the interior with a sparse, maze-like pattern of walls and water obstacles. Between five and eight fire pits are then scattered, after which a single treasure tile is placed in a location that is provably reachable from the spawn point by at least one viable path. The agent itself spawns on a randomly chosen empty floor tile and is assigned an initial facing direction (north, east, south, or west). The step counter is set to forty and begins to decrement as the episode unfolds. From the simulator’s perspective, the full world grid is stored as a two-dimensional array of immutable tile identifiers that never change until reset.

Actions:  
During play the agent may attempt seven discrete commands: move north, south, east, or west; rotate left or right by ninety degrees; or wait in place. Rotations alter only the facing direction without changing position. Translations consume a step whether or not movement succeeds; if the destination cell is a wall or water the agent remains stationary. Every action decrements the remaining-steps budget by one, ensuring temporal pressure. Invalid operations such as walking through walls are gracefully handled as null moves rather than hard errors, preserving continuity and learnability.

State Transition Rule:  
Upon receiving an action, the simulator first computes its mechanical effect—translation, rotation, or idling—then consults the grid to resolve collisions. If the resulting location is a fire tile the episode terminates immediately with failure. If it is the treasure tile the episode terminates with success. Otherwise the environment simply updates the agent’s coordinates and facing direction, decrements the step counter, and produces a new observation. The generator, obstacle layout, and tile attributes remain static throughout the episode, guaranteeing stationarity of dynamics and rewards.

Rewards:  
The reward structure is intentionally sparse and binary. A single positive reward of +1 is delivered at the exact moment the agent occupies the treasure tile while steps remain. Every other state transition, including deaths by fire or exhaustions of the step budget, returns 0. There are no negative rewards, shaping bonuses, or penalties for bumping into walls; thus the cumulative episodic return can only be either 1 (success) or 0 (failure). This simplicity focuses learning on strategic exploration and memory rather than reward engineering artifacts.

Observation:  
Each time-step the agent receives an egocentric five-by-five RGB-free symbolic window centered on its current cell, encoded as categorical values “Empty”, “Wall”, “Water”, “Fire”, or “Treasure”. Because the view scrolls with the agent, only terrain within two tiles’ Manhattan distance is visible; the rest of the map is masked, forcing partial observability and encouraging memory formation. In addition to the local grid crop, the agent is told its facing direction and the integer count of steps still available. No absolute coordinates, no global map, and no explicit list of remaining hazards are revealed. This observation design offers actionable cues—nearby obstacles, directional context, and remaining time—while preserving uncertainty that makes systematic exploration and planning necessary.

Termination:  
An episode finishes in one of three clearly defined ways: the agent stands on the treasure (success), steps into a fire pit (failure), or exhausts all forty steps (failure). There is no hidden or stochastic end condition—termination is fully determined by observable state variables, making the learning target well defined.

Special Features:  
The environment exposes an optional determinism flag allowing researchers to fix the random seed for reproducible experiments or to release random seeds for benchmarking. Except for the map layout, every aspect of the simulator—tile semantics, action effects, reward logic, and termination criteria—remains perfectly consistent across training, validation, and test splits, thereby meeting the requirement that difficulty arise from spatial complexity rather than rule variability. Because invalid moves merely waste steps instead of terminating the episode, exploration strategies can safely probe walls and water without catastrophic penalty, creating a forgiving but information-rich setting ideal for reinforcement-learning research into memory, planning, and partial observability.