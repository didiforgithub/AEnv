**Background**

The Mismatched Memory Game Environment presents a cognitively challenging twist on traditional memory card games. Set on a 4×4 grid containing 16 cards with 8 unique visual symbols (glyphs A through H), this environment deliberately subverts human intuition about visual matching. Unlike conventional memory games where identical symbols form pairs, this environment operates on an inverse semantic rule where cards that appear visually identical can never match, and only cards displaying different symbols can form valid pairs. Each episode begins with a freshly shuffled board where the underlying pairing relationships are randomly reassigned, creating a dynamic learning challenge that tests an agent's ability to discover and exploit hidden patterns while managing partial observability.

**Objective**

The agent must maximize accumulated reward by discovering and revealing as many hidden card pairs as possible within a strict 40-step budget. Success requires the agent to learn the counter-intuitive pairing rule, efficiently explore the board to gather information about card locations and symbols, maintain internal memory of previously observed cards, and strategically sequence actions to uncover matching pairs. The challenge lies in balancing exploration of unknown cards against exploitation of discovered pairing relationships, all while operating under significant time pressure and partial observability constraints.

**State Setup**

At episode initialization, the environment generates eight distinct symbol pairs following the inverse matching rule, where each symbol from the set {A, B, C, D, E, F, G, H} is randomly paired with exactly one other different symbol. These eight pairs are then randomly distributed across the 16 board positions, ensuring each symbol appears exactly twice on the board. All cards begin in a face-down state, completely hiding their symbols from the agent. The step counter initializes to 40, representing the maximum number of actions available. The board state tracking system initializes with all positions marked as unknown (value 0), ready to track the dynamic states of face-down, face-up, and solved cards throughout the episode.

**Actions**

The agent can execute one of two distinct actions per step. The Flip(x, y) action attempts to reveal the card at grid coordinates (x, y), where x and y range from 0 to 3, corresponding to the 4×4 board layout. This action succeeds only if the target card is currently face-down; attempting to flip an already face-up or previously solved card results in no state change but still consumes one step from the budget. The Wait action allows the agent to pass the current step without performing any board manipulation, which may be strategically useful when the agent needs time to process information or when optimal timing requires delaying actions. Both actions consume exactly one step from the 40-step budget, emphasizing the importance of efficient action selection.

**State Transition Rule**

State transitions follow a carefully orchestrated sequence that maintains game flow while enforcing learning constraints. When a Flip(x, y) action targets a face-down card, that card immediately transitions to face-up status, revealing its symbol and updating the observation. Before each new step begins, the environment automatically evaluates all currently face-up cards to determine if any valid pairs exist. If exactly two face-up cards display symbols that form a valid pair according to the hidden inverse mapping, both cards transition to the solved state (value 2) and become permanently removed from play, triggering a pair discovery reward. Any face-up cards that do not form valid pairs automatically return to face-down status at the start of the next step, forcing the agent to rely on memory rather than persistent visual cues. The step counter decrements by one after each action, regardless of whether the action successfully changed the board state.

**Rewards**

This environment employs a cumulative reward structure with two distinct positive reward events designed to encourage both successful pair discovery and systematic exploration. The primary reward mechanism grants +1.0 points whenever the agent successfully reveals a valid pair by having both paired cards face-up simultaneously. This substantial reward signal clearly indicates successful pattern discovery and reinforces the core objective. Additionally, the environment provides a smaller exploration bonus of +0.05 points for each card flip that reveals a symbol never previously observed during the current episode. This exploration incentive encourages comprehensive board coverage and information gathering, particularly important during early episode phases when the agent is learning the hidden pairing structure. No negative rewards or penalties exist in this system, ensuring that all agent actions either maintain or increase the cumulative score, with total possible episode rewards ranging from 0 to 9.6 points.

**Observation**

The agent receives carefully structured partial information designed to support learning while maintaining appropriate challenge levels. The core observation consists of a 4×4 grid mask where each cell contains an integer value indicating its current state: 0 for face-down cards (symbol unknown), 1 for currently face-up cards (symbol visible), and 2 for solved cards (permanently removed). Accompanying this mask, the agent observes the actual symbols displayed on all currently face-up cards, providing the critical information needed to identify potential matches. A step counter shows the remaining action budget, enabling strategic planning and urgency management. Importantly, the environment provides no historical information about previously seen but currently face-down cards, forcing the agent to develop and utilize internal memory mechanisms. The observation structure balances informativeness with difficulty by providing sufficient immediate context for decision-making while requiring memory-based integration of information across time steps.

**Termination**

Episode termination occurs under two distinct conditions, both clearly defined to support consistent learning experiences. The episode immediately ends when all eight pairs have been successfully discovered and removed from the board, representing complete task success and optimal performance. Alternatively, episodes terminate when the 40-step budget is fully exhausted, regardless of how many pairs remain undiscovered. This dual termination structure creates natural pressure for efficient exploration and decision-making while providing clear success criteria. Upon termination, the final cumulative reward represents the agent's performance score for that episode, enabling straightforward evaluation of learning progress across multiple episodes.

**Special Features**

Several unique environmental mechanics distinguish this system from standard memory games while ensuring consistent learnability. The inverse semantic rule represents the most distinctive feature, requiring agents to overcome intuitive visual matching assumptions and discover that visual similarity indicates non-matching pairs. The partial observability constraint forces agents to develop memory systems for tracking card locations and symbols across time, as the environment deliberately withholds historical information. Stochastic episode initialization ensures that each new episode presents a fresh challenge with randomized symbol pairings and spatial distributions, preventing memorization of specific board configurations while maintaining consistent underlying rules. The automatic card re-covering mechanism maintains game flow and memory pressure by returning non-matching revealed cards to face-down status at each step transition. The exploration reward system specifically encourages comprehensive board coverage, helping agents discover the full symbol set and pairing relationships efficiently. Finally, the fixed 40-step budget creates consistent temporal constraints across all episodes, ensuring that learning occurs under standardized conditions while promoting the development of efficient exploration and exploitation strategies.