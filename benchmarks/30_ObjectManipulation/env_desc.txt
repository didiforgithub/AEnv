**Background**

The Smart Home Assistant Environment simulates an embodied AI agent operating within a realistic apartment setting. The agent exists in a discrete grid-based world representing a typical residential layout containing five distinct rooms: kitchen, living room, bedroom, bathroom, and corridor. Each room maintains authentic household semantics with appropriate furniture, appliances, and objects placed according to real-world conventions. The apartment layout regenerates for each episode while preserving consistent room types and object categories, creating varied spatial challenges while maintaining familiar interaction patterns. This environment captures the complexity of domestic task execution, requiring the agent to develop spatial reasoning, object recognition, and task planning capabilities within realistic constraints.

**Objective**

The agent must successfully complete three randomly generated household chore instructions within a strict 40-step time limit. Each instruction follows natural language patterns describing realistic domestic tasks, such as relocating specific objects to appropriate locations, toggling appliances to desired states, or organizing items within storage containers. The instructions are presented at episode initialization and remain visible throughout the episode, allowing the agent to plan and prioritize task execution. Success requires completing all three assigned chores before the step budget expires, demanding efficient navigation, strategic task ordering, and effective resource management.

**State Setup**

Episode initialization begins with procedural apartment generation using a 12x12 grid framework. The system places walls to create five interconnected rooms linked by doorways, ensuring navigable pathways between all areas. Each room receives semantically appropriate furniture and appliances: refrigerators and sinks appear only in kitchens, beds exclusively in bedrooms, and bathroom fixtures solely in bathrooms. The system then populates rooms with movable objects of various types and colors, creating a diverse interaction landscape. Three chore instructions are randomly selected from predefined templates, ensuring consistent difficulty while providing task variety. The agent spawns at a random floor location with an empty inventory, facing a random cardinal direction, ready to begin task execution.

**Actions**

The agent operates through an eight-action interface designed for discrete grid navigation and object manipulation. Movement actions include MoveForward, which advances the agent one cell in their current facing direction, and TurnLeft plus TurnRight for directional changes without position updates. Object interaction capabilities consist of PickUp for acquiring objects from adjacent cells when inventory space permits, and Drop for placing carried items in front cells or containers. Appliance management occurs through ToggleAppliance, switching devices between operational states, while OpenCloseContainer manages storage accessibility. The Wait action allows deliberate inaction when strategic timing benefits task execution. All actions execute within single time steps, and illegal attempts consume time without state changes.

**State Transition Rule**

Movement actions modify agent position and orientation according to standard grid navigation rules, with wall collisions preventing illegal position changes while still consuming time steps. Object manipulation creates inventory state changes when picking up or dropping items, with container interactions requiring appropriate semantic matching between objects and storage types. Appliance toggling alternates device states between active and inactive configurations, while container operations switch accessibility states. The step counter decrements with every action regardless of success or failure, maintaining consistent temporal pressure. Object positions update when moved or dropped, and chore completion flags activate when instruction conditions are satisfied through agent actions.

**Rewards**

The environment employs a dense, cumulative reward structure using strictly non-negative values to encourage progressive task completion. Agents receive +0.3 points for initially picking up any object that serves as a target in the current instruction set, providing early positive feedback for correct object identification. Completing the final condition of any instruction yields +0.7 points, rewarding successful task execution such as placing objects in designated locations or achieving required appliance states. An additional +1.0 bonus accompanies completion of the final remaining instruction, recognizing full episode success. All other state transitions yield zero reward, maintaining the non-penalty structure while focusing reinforcement on meaningful progress toward goal achievement.

**Observation**

The agent receives carefully structured partial observability designed to balance challenge with learnability. A local 5x5 vision grid centered on the agent's position reveals immediate surroundings including walls, floors, doors, objects, appliances, and containers, with areas beyond vision range marked as unknown. Current facing direction and room identification provide spatial context essential for navigation planning. Inventory status displays any carried object with full type and color information, supporting task-relevant decision making. Visible appliances within the observation range include their current operational states, enabling informed interaction choices. A step counter maintains temporal awareness, while the complete instruction list with completion flags provides persistent goal visibility, ensuring agents can always access task requirements and track progress.

**Termination**

Episodes conclude under two distinct conditions that create meaningful completion criteria. Success termination occurs immediately upon completing all three assigned chore instructions, regardless of remaining time steps, rewarding efficient task execution. Time-based termination activates when the 40-step limit expires, creating consistent episode boundaries and resource pressure regardless of task completion status. No other conditions trigger termination, ensuring that partial appliance functionality or temporary resource unavailability does not prematurely end episodes, allowing agents to continue pursuing alternate strategies or incomplete objectives until natural conclusion points.

**Special Features**

The environment incorporates several distinctive mechanisms that enhance realism and learning potential. Semantic consistency ensures that all object-appliance and object-container interactions follow real-world logic throughout all episodes, creating stable learning targets where food items work with refrigerators and cleaning supplies belong in appropriate storage areas. Deterministic reproducibility allows identical episode generation through seed specification, supporting rigorous evaluation and debugging procedures. The discrete grid structure maintains computational efficiency while providing sufficient spatial complexity for meaningful navigation challenges. Instruction variety emerges from template-based generation that preserves difficulty consistency across episodes while preventing memorization of specific task sequences, ensuring that learned behaviors generalize beyond training distributions.